#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„æ£€ç´¢è¯„ä¼°å·¥å…·
æ”¯æŒä¸¤ç§QAæ•°æ®æ ¼å¼ï¼š
1. æ–¹æ¡ˆAï¼šæ¯ä¸ªé—®é¢˜1ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆdoc_idå­—æ®µï¼‰
2. æ–¹æ¡ˆBï¼šæ¯ä¸ªé—®é¢˜å¤šä¸ªç›¸å…³æ–‡æ¡£ï¼ˆrelevant_doc_idså­—æ®µï¼‰
"""

import json
import numpy as np
from typing import List, Dict, Any, Set, Tuple, Optional
from pathlib import Path
import sys
import argparse
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))


class UniversalRetrievalEvaluator:
    """
    é€šç”¨æ£€ç´¢è¯„ä¼°å™¨ - æ”¯æŒå•æ–‡æ¡£å’Œå¤šæ–‡æ¡£æ ¼å¼
    """
    
    def __init__(self, retriever=None):
        self.retriever = retriever
    
    def load_qa_dataset(self, file_path: str) -> Tuple[List[Dict[str, Any]], str]:
        """
        åŠ è½½QAæ•°æ®é›†å¹¶è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        
        Returns:
            (æ•°æ®åˆ—è¡¨, æ ¼å¼ç±»å‹)
        """
        qa_pairs = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                qa_pairs.append(json.loads(line.strip()))
        
        # æ£€æµ‹æ ¼å¼
        if qa_pairs and 'relevant_doc_ids' in qa_pairs[0]:
            format_type = "multi_doc"  # æ–¹æ¡ˆBï¼šå¤šæ–‡æ¡£æ ¼å¼
        else:
            format_type = "single_doc"  # æ–¹æ¡ˆAï¼šå•æ–‡æ¡£æ ¼å¼
        
        return qa_pairs, format_type
    
    def normalize_qa_dataset(self, qa_dataset: List[Dict[str, Any]], 
                           format_type: str) -> List[Dict[str, Any]]:
        """
        å°†æ•°æ®é›†æ ‡å‡†åŒ–ä¸ºç»Ÿä¸€æ ¼å¼
        """
        normalized_dataset = []
        
        for qa in qa_dataset:
            normalized_qa = qa.copy()
            
            if format_type == "single_doc":
                # æ–¹æ¡ˆAï¼šå•æ–‡æ¡£æ ¼å¼
                relevant_docs = {qa['doc_id']} if 'doc_id' in qa else set()
            else:
                # æ–¹æ¡ˆBï¼šå¤šæ–‡æ¡£æ ¼å¼
                relevant_docs = set(qa.get('relevant_doc_ids', []))
            
            normalized_qa['relevant_docs'] = relevant_docs
            normalized_qa['num_relevant_docs'] = len(relevant_docs)
            
            normalized_dataset.append(normalized_qa)
        
        return normalized_dataset
    
    def calculate_metrics(self, results: List[Dict[str, Any]], 
                         k_values: List[int] = [1, 3, 5, 10]) -> Dict[str, float]:
        """
        è®¡ç®—å…¨é¢çš„æ£€ç´¢æŒ‡æ ‡
        """
        metrics = {}
        total_queries = len(results)
        
        if total_queries == 0:
            return metrics
        
        # Recall@K
        for k in k_values:
            recall_at_k = 0
            for result in results:
                relevant_docs = result['relevant_docs']
                retrieved_docs_at_k = set(result['retrieved_docs'][:k])
                
                if len(relevant_docs) > 0:
                    recall = len(relevant_docs & retrieved_docs_at_k) / len(relevant_docs)
                    recall_at_k += recall
            
            metrics[f'Recall@{k}'] = recall_at_k / total_queries
        
        # Precision@K
        for k in k_values:
            precision_at_k = 0
            for result in results:
                relevant_docs = result['relevant_docs']
                retrieved_docs_at_k = set(result['retrieved_docs'][:k])
                
                if len(retrieved_docs_at_k) > 0:
                    precision = len(relevant_docs & retrieved_docs_at_k) / len(retrieved_docs_at_k)
                    precision_at_k += precision
            
            metrics[f'Precision@{k}'] = precision_at_k / total_queries
        
        # F1@K
        for k in k_values:
            recall_k = metrics.get(f'Recall@{k}', 0)
            precision_k = metrics.get(f'Precision@{k}', 0)
            
            if recall_k + precision_k > 0:
                f1_k = 2 * (recall_k * precision_k) / (recall_k + precision_k)
            else:
                f1_k = 0.0
            
            metrics[f'F1@{k}'] = f1_k
        
        # MRR (Mean Reciprocal Rank)
        mrr = 0
        for result in results:
            relevant_docs = result['relevant_docs']
            retrieved_docs = result['retrieved_docs']
            
            for i, doc_id in enumerate(retrieved_docs):
                if doc_id in relevant_docs:
                    mrr += 1.0 / (i + 1)
                    break
        
        metrics['MRR'] = mrr / total_queries
        
        # MAP (Mean Average Precision)
        map_score = 0
        for result in results:
            relevant_docs = result['relevant_docs']
            retrieved_docs = result['retrieved_docs']
            
            if len(relevant_docs) == 0:
                continue
            
            precision_sum = 0
            relevant_retrieved = 0
            
            for i, doc_id in enumerate(retrieved_docs):
                if doc_id in relevant_docs:
                    relevant_retrieved += 1
                    precision_at_i = relevant_retrieved / (i + 1)
                    precision_sum += precision_at_i
            
            if relevant_retrieved > 0:
                avg_precision = precision_sum / len(relevant_docs)
                map_score += avg_precision
        
        metrics['MAP'] = map_score / total_queries
        
        # NDCG@K
        for k in k_values:
            ndcg_at_k = 0
            for result in results:
                relevant_docs = result['relevant_docs']
                retrieved_docs = result['retrieved_docs'][:k]
                
                dcg = 0
                idcg = 0
                
                # DCG
                for i, doc_id in enumerate(retrieved_docs):
                    relevance = 1 if doc_id in relevant_docs else 0
                    dcg += relevance / np.log2(i + 2)
                
                # IDCG
                for i in range(min(k, len(relevant_docs))):
                    idcg += 1 / np.log2(i + 2)
                
                if idcg > 0:
                    ndcg_at_k += dcg / idcg
            
            metrics[f'NDCG@{k}'] = ndcg_at_k / total_queries
        
        # Hit Rate
        hits = 0
        for result in results:
            relevant_docs = result['relevant_docs']
            retrieved_docs = set(result['retrieved_docs'])
            if len(relevant_docs & retrieved_docs) > 0:
                hits += 1
        
        metrics['Hit_Rate'] = hits / total_queries
        
        # Coverage (æ£€ç´¢åˆ°çš„å”¯ä¸€ç›¸å…³æ–‡æ¡£æ•° / æ€»ç›¸å…³æ–‡æ¡£æ•°)
        all_relevant_docs = set()
        retrieved_relevant_docs = set()
        
        for result in results:
            all_relevant_docs.update(result['relevant_docs'])
            retrieved_docs = set(result['retrieved_docs'])
            retrieved_relevant_docs.update(result['relevant_docs'] & retrieved_docs)
        
        if len(all_relevant_docs) > 0:
            metrics['Coverage'] = len(retrieved_relevant_docs) / len(all_relevant_docs)
        else:
            metrics['Coverage'] = 0.0
        
        return metrics
    
    def evaluate_with_mock_retriever(self, normalized_dataset: List[Dict[str, Any]], 
                                   format_type: str,
                                   num_retrieved: int = 10) -> Dict[str, float]:
        """
        ä½¿ç”¨æ¨¡æ‹Ÿæ£€ç´¢å™¨è¿›è¡Œè¯„ä¼°
        """
        results = []
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„æ–‡æ¡£ID
        all_doc_ids = set()
        for qa in normalized_dataset:
            all_doc_ids.update(qa['relevant_docs'])
        all_doc_ids = list(all_doc_ids)
        
        import random
        random.seed(42)
        
        for qa in normalized_dataset:
            query = qa['question']
            relevant_docs = qa['relevant_docs']
            
            # æ ¹æ®æ ¼å¼ç±»å‹è°ƒæ•´æ¨¡æ‹Ÿç­–ç•¥
            if format_type == "single_doc":
                # å•æ–‡æ¡£ï¼š50%æ¦‚ç‡ç›¸å…³æ–‡æ¡£åœ¨å‰3ä½
                success_rate = 0.5
                early_positions = 3
            else:
                # å¤šæ–‡æ¡£ï¼š70%æ¦‚ç‡è‡³å°‘ä¸€ä¸ªç›¸å…³æ–‡æ¡£åœ¨å‰5ä½
                success_rate = 0.7
                early_positions = 5
            
            retrieved_docs = []
            other_docs = [doc for doc in all_doc_ids if doc not in relevant_docs]
            random.shuffle(other_docs)
            
            if random.random() < success_rate:
                # æˆåŠŸæƒ…å†µ
                if format_type == "single_doc":
                    # å•æ–‡æ¡£ï¼šå°†å”¯ä¸€ç›¸å…³æ–‡æ¡£æ”¾åœ¨å‰é¢
                    position = random.randint(0, min(early_positions-1, num_retrieved-1))
                    for i in range(num_retrieved):
                        if i == position and relevant_docs:
                            retrieved_docs.append(list(relevant_docs)[0])
                        else:
                            if other_docs:
                                retrieved_docs.append(other_docs.pop())
                else:
                    # å¤šæ–‡æ¡£ï¼šéšæœºé€‰æ‹©1-2ä¸ªç›¸å…³æ–‡æ¡£æ”¾åœ¨å‰é¢
                    selected_relevant = random.sample(
                        list(relevant_docs), 
                        min(random.randint(1, 2), len(relevant_docs))
                    )
                    
                    for i in range(num_retrieved):
                        if i < len(selected_relevant):
                            retrieved_docs.append(selected_relevant[i])
                        else:
                            if other_docs:
                                retrieved_docs.append(other_docs.pop())
            else:
                # å¤±è´¥æƒ…å†µ
                retrieved_docs = other_docs[:num_retrieved]
                
                # å°æ¦‚ç‡åœ¨åé¢åŒ…å«ä¸€ä¸ªç›¸å…³æ–‡æ¡£
                if random.random() < 0.25 and relevant_docs and len(retrieved_docs) > early_positions:
                    pos = random.randint(early_positions, len(retrieved_docs)-1)
                    retrieved_docs[pos] = random.choice(list(relevant_docs))
            
            results.append({
                'query': query,
                'relevant_docs': relevant_docs,
                'retrieved_docs': retrieved_docs[:num_retrieved]
            })
        
        return self.calculate_metrics(results)
    
    def analyze_dataset_characteristics(self, normalized_dataset: List[Dict[str, Any]], 
                                     format_type: str) -> Dict[str, Any]:
        """
        åˆ†ææ•°æ®é›†ç‰¹å¾
        """
        characteristics = {
            'total_queries': len(normalized_dataset),
            'format_type': format_type,
            'categories': {},
            'years': {},
            'relevant_docs_stats': {}
        }
        
        relevant_doc_counts = []
        
        for qa in normalized_dataset:
            # ç±»åˆ«ç»Ÿè®¡
            cat = qa.get('category', 'unknown')
            characteristics['categories'][cat] = characteristics['categories'].get(cat, 0) + 1
            
            # å¹´ä»½ç»Ÿè®¡
            year = qa.get('year', 'unknown')
            characteristics['years'][year] = characteristics['years'].get(year, 0) + 1
            
            # ç›¸å…³æ–‡æ¡£æ•°é‡
            relevant_doc_counts.append(qa['num_relevant_docs'])
        
        # ç›¸å…³æ–‡æ¡£ç»Ÿè®¡
        if relevant_doc_counts:
            characteristics['relevant_docs_stats'] = {
                'mean': float(np.mean(relevant_doc_counts)),
                'std': float(np.std(relevant_doc_counts)),
                'min': int(min(relevant_doc_counts)),
                'max': int(max(relevant_doc_counts)),
                'median': float(np.median(relevant_doc_counts))
            }
        
        return characteristics
    
    def print_comprehensive_results(self, metrics: Dict[str, float], 
                                  characteristics: Dict[str, Any],
                                  title: str = "æ£€ç´¢è¯„ä¼°ç»“æœ"):
        """
        æ‰“å°å…¨é¢çš„è¯„ä¼°ç»“æœ
        """
        print("=" * 80)
        print(f"{title}")
        print("=" * 80)
        
        # æ•°æ®é›†ä¿¡æ¯
        print(f"æ•°æ®é›†ç±»å‹: {characteristics['format_type']}")
        print(f"æŸ¥è¯¢æ€»æ•°: {characteristics['total_queries']}")
        
        rel_stats = characteristics.get('relevant_docs_stats', {})
        if rel_stats:
            print(f"ç›¸å…³æ–‡æ¡£æ•°: å¹³å‡{rel_stats['mean']:.2f} (èŒƒå›´: {rel_stats['min']}-{rel_stats['max']})")
        
        # æ ¸å¿ƒæŒ‡æ ‡
        print("\nğŸ“Š æ ¸å¿ƒæ£€ç´¢æŒ‡æ ‡:")
        print("-" * 50)
        
        core_metrics = [
            ('Recall@1', 'R@1'),
            ('Recall@3', 'R@3'), 
            ('Recall@5', 'R@5'),
            ('Precision@3', 'P@3'),
            ('F1@3', 'F1@3'),
            ('MRR', 'MRR'),
            ('MAP', 'MAP'),
            ('Hit_Rate', 'Hit')
        ]
        
        for full_name, short_name in core_metrics:
            if full_name in metrics:
                value = metrics[full_name]
                print(f"{short_name:8}: {value:.4f} ({value*100:5.1f}%)")
        
        # å®Œæ•´RecallæŒ‡æ ‡
        print("\nğŸ¯ å¬å›ç‡æŒ‡æ ‡ (Recall@K):")
        print("-" * 50)
        recall_metrics = [(k, v) for k, v in metrics.items() if k.startswith('Recall@')]
        for metric, value in sorted(recall_metrics):
            k = metric.split('@')[1]
            print(f"R@{k:2}: {value:.4f} ({value*100:5.1f}%)")
        
        # å®Œæ•´PrecisionæŒ‡æ ‡  
        print("\nğŸ¯ ç²¾ç¡®ç‡æŒ‡æ ‡ (Precision@K):")
        print("-" * 50)
        precision_metrics = [(k, v) for k, v in metrics.items() if k.startswith('Precision@')]
        for metric, value in sorted(precision_metrics):
            k = metric.split('@')[1]
            print(f"P@{k:2}: {value:.4f} ({value*100:5.1f}%)")
        
        # é«˜çº§æŒ‡æ ‡
        print("\nğŸ† é«˜çº§æŒ‡æ ‡:")
        print("-" * 50)
        advanced_metrics = ['NDCG@1', 'NDCG@3', 'NDCG@5', 'Coverage']
        for metric in advanced_metrics:
            if metric in metrics:
                value = metrics[metric]
                print(f"{metric:10}: {value:.4f} ({value*100:5.1f}%)")
        
        # æ€§èƒ½è¯„ä¼°
        print("\nğŸ“ˆ æ€§èƒ½è¯„ä¼°:")
        print("-" * 50)
        
        # æ ¹æ®ä¸åŒæ ¼å¼ç»™å‡ºä¸åŒå»ºè®®
        format_type = characteristics['format_type']
        
        if format_type == "single_doc":
            # å•æ–‡æ¡£æ ¼å¼è¯„ä¼°
            r1 = metrics.get('Recall@1', 0)
            mrr = metrics.get('MRR', 0)
            
            if r1 > 0.8:
                print("âœ… Recall@1 > 80%: ä¼˜ç§€çš„å•æ–‡æ¡£æ£€ç´¢æ€§èƒ½")
            elif r1 > 0.6:
                print("âš¡ Recall@1 > 60%: è‰¯å¥½çš„å•æ–‡æ¡£æ£€ç´¢æ€§èƒ½")  
            elif r1 > 0.4:
                print("âš ï¸  Recall@1 > 40%: å•æ–‡æ¡£æ£€ç´¢éœ€è¦ä¼˜åŒ–")
            else:
                print("âŒ Recall@1 < 40%: å•æ–‡æ¡£æ£€ç´¢æ€§èƒ½è¾ƒå·®")
                
            if mrr > 0.7:
                print("âœ… MRR > 0.7: ç›¸å…³æ–‡æ¡£æ’åå¾ˆå¥½")
            elif mrr > 0.5:
                print("âš¡ MRR > 0.5: ç›¸å…³æ–‡æ¡£æ’åè‰¯å¥½")
            else:
                print("âš ï¸  MRR < 0.5: ç›¸å…³æ–‡æ¡£æ’åéœ€è¦æ”¹è¿›")
                
        else:
            # å¤šæ–‡æ¡£æ ¼å¼è¯„ä¼°
            r5 = metrics.get('Recall@5', 0)
            f1_3 = metrics.get('F1@3', 0)
            map_score = metrics.get('MAP', 0)
            
            if r5 > 0.8:
                print("âœ… Recall@5 > 80%: ä¼˜ç§€çš„å¤šæ–‡æ¡£æ£€ç´¢è¦†ç›–ç‡")
            elif r5 > 0.6:
                print("âš¡ Recall@5 > 60%: è‰¯å¥½çš„å¤šæ–‡æ¡£æ£€ç´¢è¦†ç›–ç‡")
            elif r5 > 0.4:
                print("âš ï¸  Recall@5 > 40%: å¤šæ–‡æ¡£æ£€ç´¢è¦†ç›–ç‡éœ€è¦æ”¹è¿›") 
            else:
                print("âŒ Recall@5 < 40%: å¤šæ–‡æ¡£æ£€ç´¢è¦†ç›–ç‡è¾ƒå·®")
                
            if f1_3 > 0.6:
                print("âœ… F1@3 > 60%: ä¼˜ç§€çš„ç²¾ç¡®ç‡å¬å›ç‡å¹³è¡¡")
            elif f1_3 > 0.4:
                print("âš¡ F1@3 > 40%: è‰¯å¥½çš„ç²¾ç¡®ç‡å¬å›ç‡å¹³è¡¡")
            else:
                print("âš ï¸  F1@3 < 40%: éœ€è¦å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡")
                
            if map_score > 0.5:
                print("âœ… MAP > 0.5: ä¼˜ç§€çš„å¹³å‡ç²¾ç¡®ç‡")
            elif map_score > 0.3:
                print("âš¡ MAP > 0.3: è‰¯å¥½çš„å¹³å‡ç²¾ç¡®ç‡")
            else:
                print("âš ï¸  MAP < 0.3: å¹³å‡ç²¾ç¡®ç‡éœ€è¦æ”¹è¿›")
        
        # ç±»åˆ«ç»Ÿè®¡
        print("\nğŸ“‹ ç±»åˆ«åˆ†å¸ƒ:")
        print("-" * 50)
        categories = characteristics.get('categories', {})
        for cat, count in sorted(categories.items()):
            percentage = (count / characteristics['total_queries']) * 100
            print(f"{cat:12}: {count:3d} ({percentage:4.1f}%)")


def compare_evaluations(results_a: Dict[str, float], 
                       results_b: Dict[str, float],
                       char_a: Dict[str, Any],
                       char_b: Dict[str, Any]):
    """
    æ¯”è¾ƒä¸¤ç§è¯„ä¼°æ–¹æ¡ˆçš„ç»“æœ
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    print(f"æ–¹æ¡ˆA ({char_a['format_type']}): {char_a['total_queries']} ä¸ªæŸ¥è¯¢")
    print(f"æ–¹æ¡ˆB ({char_b['format_type']}): {char_b['total_queries']} ä¸ªæŸ¥è¯¢")
    
    # å¯¹æ¯”æ ¸å¿ƒæŒ‡æ ‡
    comparison_metrics = [
        'Recall@1', 'Recall@3', 'Recall@5', 
        'Precision@3', 'F1@3', 'MRR', 'MAP'
    ]
    
    print(f"\n{'æŒ‡æ ‡':<12} {'æ–¹æ¡ˆA':<10} {'æ–¹æ¡ˆB':<10} {'å·®å€¼':<10} {'æ”¹è¿›':<8}")
    print("-" * 55)
    
    for metric in comparison_metrics:
        val_a = results_a.get(metric, 0)
        val_b = results_b.get(metric, 0)
        diff = val_b - val_a
        
        if abs(diff) < 0.001:
            improvement = "="
        elif diff > 0:
            improvement = f"+{diff/val_a*100:.1f}%" if val_a > 0 else "N/A"
        else:
            improvement = f"{diff/val_a*100:.1f}%" if val_a > 0 else "N/A"
        
        print(f"{metric:<12} {val_a:<10.4f} {val_b:<10.4f} {diff:>+8.4f} {improvement:>7}")
    
    # å»ºè®®
    print("\nğŸ’¡ å¯¹æ¯”åˆ†æ:")
    print("-" * 50)
    
    rel_stats_a = char_a.get('relevant_docs_stats', {})
    rel_stats_b = char_b.get('relevant_docs_stats', {}) 
    
    if rel_stats_b.get('mean', 1) > rel_stats_a.get('mean', 1):
        print("â€¢ æ–¹æ¡ˆBæä¾›äº†æ›´ä¸°å¯Œçš„ç›¸å…³æ–‡æ¡£æ ‡æ³¨")
        print("â€¢ æ–¹æ¡ˆBæ›´é€‚åˆè¯„ä¼°æ£€ç´¢ç³»ç»Ÿçš„å…¨é¢æ€§èƒ½")
        print("â€¢ æ–¹æ¡ˆBæ”¯æŒæ›´å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡")
    
    if results_b.get('Recall@5', 0) > results_a.get('Recall@1', 0):
        print("â€¢ å¤šæ–‡æ¡£æ ‡æ³¨æé«˜äº†æ£€ç´¢è¯„ä¼°çš„å¯é æ€§")
    
    print("â€¢ ä¸¤ç§æ–¹æ¡ˆå¯ä»¥äº’è¡¥ä½¿ç”¨ï¼š")
    print("  - æ–¹æ¡ˆAé€‚åˆå¿«é€Ÿè¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•")
    print("  - æ–¹æ¡ˆBé€‚åˆæ·±å…¥åˆ†æå’Œç³»ç»Ÿä¼˜åŒ–")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é€šç”¨æ£€ç´¢è¯„ä¼°å·¥å…·')
    parser.add_argument('--qa_file_a', type=str, 
                       help='æ–¹æ¡ˆAçš„QAæ–‡ä»¶è·¯å¾„ï¼ˆå•æ–‡æ¡£æ ¼å¼ï¼‰')
    parser.add_argument('--qa_file_b', type=str,
                       help='æ–¹æ¡ˆBçš„QAæ–‡ä»¶è·¯å¾„ï¼ˆå¤šæ–‡æ¡£æ ¼å¼ï¼‰')
    parser.add_argument('--output_dir', type=str, default='results',
                       help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--num_retrieved', type=int, default=10,
                       help='æ£€ç´¢æ–‡æ¡£æ•°é‡')
    
    args = parser.parse_args()
    
    # é»˜è®¤æ–‡ä»¶è·¯å¾„
    project_root = Path(__file__).parent.parent
    
    if not args.qa_file_a:
        args.qa_file_a = str(project_root / "data" / "test" / "test_qa.jsonl")
    
    if not args.qa_file_b:
        args.qa_file_b = str(project_root / "data" / "test" / "test_qa_expanded.jsonl")
    
    output_dir = Path(args.output_dir)
    if not output_dir.is_absolute():
        output_dir = project_root / output_dir
    output_dir.mkdir(exist_ok=True)
    
    evaluator = UniversalRetrievalEvaluator()
    
    results_summary = {
        'evaluation_params': {
            'num_retrieved': args.num_retrieved
        },
        'results': {}
    }
    
    # è¯„ä¼°æ–¹æ¡ˆA
    if Path(args.qa_file_a).exists():
        print("ğŸ” è¯„ä¼°æ–¹æ¡ˆA...")
        qa_dataset_a, format_a = evaluator.load_qa_dataset(args.qa_file_a)
        normalized_a = evaluator.normalize_qa_dataset(qa_dataset_a, format_a)
        characteristics_a = evaluator.analyze_dataset_characteristics(normalized_a, format_a)
        
        metrics_a = evaluator.evaluate_with_mock_retriever(
            normalized_a, format_a, args.num_retrieved
        )
        
        evaluator.print_comprehensive_results(
            metrics_a, characteristics_a, 
            "æ–¹æ¡ˆA: å•æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°ç»“æœ"
        )
        
        results_summary['results']['plan_a'] = {
            'characteristics': characteristics_a,
            'metrics': metrics_a
        }
    else:
        print(f"âš ï¸  æ–¹æ¡ˆAæ–‡ä»¶ä¸å­˜åœ¨: {args.qa_file_a}")
        metrics_a, characteristics_a = None, None
    
    # è¯„ä¼°æ–¹æ¡ˆB
    if Path(args.qa_file_b).exists():
        print("\nğŸ” è¯„ä¼°æ–¹æ¡ˆB...")
        qa_dataset_b, format_b = evaluator.load_qa_dataset(args.qa_file_b)
        normalized_b = evaluator.normalize_qa_dataset(qa_dataset_b, format_b)
        characteristics_b = evaluator.analyze_dataset_characteristics(normalized_b, format_b)
        
        metrics_b = evaluator.evaluate_with_mock_retriever(
            normalized_b, format_b, args.num_retrieved
        )
        
        evaluator.print_comprehensive_results(
            metrics_b, characteristics_b,
            "æ–¹æ¡ˆB: å¤šæ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°ç»“æœ"
        )
        
        results_summary['results']['plan_b'] = {
            'characteristics': characteristics_b,
            'metrics': metrics_b
        }
    else:
        print(f"âš ï¸  æ–¹æ¡ˆBæ–‡ä»¶ä¸å­˜åœ¨: {args.qa_file_b}")
        metrics_b, characteristics_b = None, None
    
    # å¯¹æ¯”åˆ†æ
    if metrics_a and metrics_b:
        compare_evaluations(metrics_a, metrics_b, characteristics_a, characteristics_b)
        
        results_summary['comparison'] = {
            'summary': "æ–¹æ¡ˆBåœ¨å¤šæ–‡æ¡£ç›¸å…³æ€§ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†æ–¹æ¡ˆAæ›´é€‚åˆå¿«é€Ÿè¯„ä¼°"
        }
    
    # ä¿å­˜ç»“æœ
    results_file = output_dir / "universal_retrieval_evaluation.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ä½¿ç”¨å»ºè®®
    print("\n" + "=" * 80)
    print("ğŸ¯ ä½¿ç”¨å»ºè®®")
    print("=" * 80)
    print("1. å¿«é€ŸéªŒè¯: ä½¿ç”¨æ–¹æ¡ˆAè¿›è¡ŒåŸºç¡€æ£€ç´¢æ€§èƒ½éªŒè¯")
    print("2. æ·±åº¦åˆ†æ: ä½¿ç”¨æ–¹æ¡ˆBè¿›è¡Œå…¨é¢çš„æ£€ç´¢ç³»ç»Ÿè¯„ä¼°")
    print("3. ç³»ç»Ÿä¼˜åŒ–: ç»“åˆä¸¤ç§æ–¹æ¡ˆçš„ç»“æœè¿›è¡Œæ£€ç´¢å‚æ•°è°ƒä¼˜")
    print("4. ç”Ÿäº§éƒ¨ç½²: åœ¨çœŸå®æ£€ç´¢å™¨ä¸Šè¿è¡Œä¸¤ç§è¯„ä¼°ä»¥éªŒè¯æ€§èƒ½")


if __name__ == "__main__":
    main()