training:
  batch_size: 16
  learning_rate: 0.00002
  num_epochs: 10
  warmup_steps: 500
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  
  optimizer:
    type: adamw
    betas: [0.9, 0.999]
    epsilon: 0.00000001
    weight_decay: 0.01
  
  scheduler:
    type: linear_warmup
    warmup_ratio: 0.1
  
  checkpointing:
    save_steps: 1000
    save_total_limit: 3
    load_best_model_at_end: true
  
  evaluation:
    eval_steps: 500
    eval_strategy: steps
    metric_for_best_model: eval_loss
  
  logging:
    log_steps: 100
    log_level: info